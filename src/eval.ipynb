{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Analysis\n",
    "This notebook loads predictions from all models (SARIMA, SIR, SEIR) and visualizes their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure figure size\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "pred_dir = Path('../predictions')\n",
    "\n",
    "# Load predictions\n",
    "models = {}\n",
    "\n",
    "# SARIMA\n",
    "sarima_path = pred_dir / 'sarima_predictions.csv'\n",
    "if sarima_path.exists():\n",
    "    models['SARIMA'] = pd.read_csv(sarima_path)\n",
    "    models['SARIMA']['date'] = pd.to_datetime(models['SARIMA']['date'])\n",
    "    print(f\"Loaded SARIMA predictions: {len(models['SARIMA'])} rows\")\n",
    "else:\n",
    "    print(f\"Warning: {sarima_path} not found\")\n",
    "\n",
    "# SIR\n",
    "sir_path = pred_dir / 'sir_predictions.csv'\n",
    "if sir_path.exists():\n",
    "    models['SIR'] = pd.read_csv(sir_path)\n",
    "    models['SIR']['date'] = pd.to_datetime(models['SIR']['date'])\n",
    "    print(f\"Loaded SIR predictions: {len(models['SIR'])} rows\")\n",
    "else:\n",
    "    print(f\"Warning: {sir_path} not found\")\n",
    "\n",
    "# SEIR\n",
    "seir_path = pred_dir / 'seir_predictions.csv'\n",
    "if seir_path.exists():\n",
    "    models['SEIR'] = pd.read_csv(seir_path)\n",
    "    models['SEIR']['date'] = pd.to_datetime(models['SEIR']['date'])\n",
    "    print(f\"Loaded SEIR predictions: {len(models['SEIR'])} rows\")\n",
    "else:\n",
    "    print(f\"Warning: {seir_path} not found\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each model and region\n",
    "regions = ['CA', 'MA', 'NY', 'nat']\n",
    "metrics_summary = []\n",
    "\n",
    "for model_name, df in models.items():\n",
    "    for region in regions:\n",
    "        region_data = df[df['region'] == region]\n",
    "        if len(region_data) > 0:\n",
    "            metrics = calculate_metrics(\n",
    "                region_data['true_ili'].values,\n",
    "                region_data['predicted_ili'].values\n",
    "            )\n",
    "            metrics_summary.append({\n",
    "                'Model': model_name,\n",
    "                'Region': region,\n",
    "                **metrics\n",
    "            })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional PRedictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for each region\n",
    "for region in regions:\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Get actual values (from first available model)\n",
    "    first_model = list(models.keys())[0]\n",
    "    region_data = models[first_model][models[first_model]['region'] == region]\n",
    "    \n",
    "    if len(region_data) > 0:\n",
    "        # Plot actual values\n",
    "        ax.plot(region_data['date'], region_data['true_ili'], \n",
    "                'k-', linewidth=2, label='Actual', marker='o', markersize=4)\n",
    "        \n",
    "        # Plot predictions from each model\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        markers = ['s', '^', 'D']\n",
    "        \n",
    "        for idx, (model_name, df) in enumerate(models.items()):\n",
    "            model_region_data = df[df['region'] == region]\n",
    "            if len(model_region_data) > 0:\n",
    "                ax.plot(model_region_data['date'], model_region_data['predicted_ili'],\n",
    "                        linewidth=2, label=model_name, alpha=0.7,\n",
    "                        marker=markers[idx % len(markers)], markersize=4,\n",
    "                        color=colors[idx % len(colors)])\n",
    "        \n",
    "        ax.set_title(f'Influenza Forecasts - {region} (2024)', fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Date', fontsize=12)\n",
    "        ax.set_ylabel('ILI Rate', fontsize=12)\n",
    "        ax.legend(fontsize=10, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(f'../predictions/predictions_{region}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Plot saved: predictions/predictions_{region}.png\")\n",
    "    else:\n",
    "        print(f\"No data available for region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals and plot error distribution\n",
    "for region in regions:\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, df) in enumerate(models.items()):\n",
    "        region_data = df[df['region'] == region]\n",
    "        \n",
    "        if len(region_data) > 0:\n",
    "            # Calculate residuals\n",
    "            residuals = region_data['true_ili'] - region_data['predicted_ili']\n",
    "            \n",
    "            # Plot histogram\n",
    "            axes[idx].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "            axes[idx].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "            axes[idx].set_title(f'{model_name} - {region}\\nMean Error: {residuals.mean():.4f}',\n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel('Residual (True - Predicted)', fontsize=10)\n",
    "            axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "            axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../predictions/residuals_{region}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Residuals plot saved: predictions/residuals_{region}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate week-by-week errors\n",
    "for region in regions:\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    for model_name, df in models.items():\n",
    "        region_data = df[df['region'] == region].copy()\n",
    "        \n",
    "        if len(region_data) > 0:\n",
    "            # Calculate absolute error\n",
    "            region_data['abs_error'] = np.abs(region_data['true_ili'] - region_data['predicted_ili'])\n",
    "            \n",
    "            ax.plot(region_data['date'], region_data['abs_error'],\n",
    "                   linewidth=2, label=model_name, marker='o', markersize=4, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'Absolute Error Over Time - {region}', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Absolute Error', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../predictions/error_timeline_{region}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Error timeline plot saved: predictions/error_timeline_{region}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Model Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rank for each model across all metrics and regions\n",
    "ranking_metrics = ['RMSE', 'MAE', 'MAPE']\n",
    "rankings = []\n",
    "\n",
    "for region in regions:\n",
    "    region_metrics = metrics_df[metrics_df['Region'] == region]\n",
    "    \n",
    "    for metric in ranking_metrics:\n",
    "        # Lower is better for RMSE, MAE, MAPE\n",
    "        region_metrics_sorted = region_metrics.sort_values(metric)\n",
    "        \n",
    "        for rank, row in enumerate(region_metrics_sorted.itertuples(), 1):\n",
    "            rankings.append({\n",
    "                'Model': row.Model,\n",
    "                'Region': region,\n",
    "                'Metric': metric,\n",
    "                'Rank': rank\n",
    "            })\n",
    "\n",
    "ranking_df = pd.DataFrame(rankings)\n",
    "\n",
    "# Calculate average rank per model\n",
    "avg_ranking = ranking_df.groupby('Model')['Rank'].mean().sort_values()\n",
    "\n",
    "print(\"\\nOverall Model Ranking (lower is better):\")\n",
    "print(avg_ranking)\n",
    "\n",
    "# Plot ranking\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "avg_ranking.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Average Rank', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Overall Model Performance Ranking', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../predictions/model_ranking.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRanking plot saved to: predictions/model_ranking.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
